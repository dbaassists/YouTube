{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcf76bf0-4c3a-4dc2-ae15-d238861d0bed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----+\n|     Clube|            Titulo| Ano|\n+----------+------------------+----+\n|Fluminense|Campeonato Carioca|1906|\n|Fluminense|Campeonato Carioca|1907|\n|Fluminense|Campeonato Carioca|1908|\n|Fluminense|Campeonato Carioca|1909|\n|Fluminense|Campeonato Carioca|1911|\n|Fluminense|Campeonato Carioca|1917|\n|Fluminense|Campeonato Carioca|1918|\n|Fluminense|Campeonato Carioca|1919|\n|Fluminense|Campeonato Carioca|1924|\n|Fluminense|Campeonato Carioca|1936|\n|Fluminense|Campeonato Carioca|1937|\n|Fluminense|Campeonato Carioca|1938|\n|Fluminense|Campeonato Carioca|1940|\n|Fluminense|Campeonato Carioca|1941|\n|Fluminense|Campeonato Carioca|1946|\n|Fluminense|Campeonato Carioca|1951|\n|Fluminense|Campeonato Carioca|1959|\n|Fluminense|Campeonato Carioca|1964|\n|Fluminense|Campeonato Carioca|1969|\n|Fluminense|Campeonato Carioca|1971|\n+----------+------------------+----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "dados = [('Fluminense', 'Campeonato Carioca', 1906)\n",
    ",('Fluminense', 'Campeonato Carioca', 1907)\n",
    ",('Fluminense', 'Campeonato Carioca', 1908)\n",
    ",('Fluminense', 'Campeonato Carioca', 1909)\n",
    ",('Fluminense', 'Campeonato Carioca', 1911)\n",
    ",('Fluminense', 'Campeonato Carioca', 1917)\n",
    ",('Fluminense', 'Campeonato Carioca', 1918)\n",
    ",('Fluminense', 'Campeonato Carioca', 1919)\n",
    ",('Fluminense', 'Campeonato Carioca', 1924)\n",
    ",('Fluminense', 'Campeonato Carioca', 1936)\n",
    ",('Fluminense', 'Campeonato Carioca', 1937)\n",
    ",('Fluminense', 'Campeonato Carioca', 1938)\n",
    ",('Fluminense', 'Campeonato Carioca', 1940)\n",
    ",('Fluminense', 'Campeonato Carioca', 1941)\n",
    ",('Fluminense', 'Campeonato Carioca', 1946)\n",
    ",('Fluminense', 'Campeonato Carioca', 1951)\n",
    ",('Fluminense', 'Campeonato Carioca', 1959)\n",
    ",('Fluminense', 'Campeonato Carioca', 1964)\n",
    ",('Fluminense', 'Campeonato Carioca', 1969)\n",
    ",('Fluminense', 'Campeonato Carioca', 1971)\n",
    ",('Fluminense', 'Campeonato Carioca', 1973)\n",
    ",('Fluminense', 'Campeonato Carioca', 1975)\n",
    ",('Fluminense', 'Campeonato Carioca', 1976)\n",
    ",('Fluminense', 'Campeonato Carioca', 1980)\n",
    ",('Fluminense', 'Campeonato Carioca', 1983)\n",
    ",('Fluminense', 'Campeonato Carioca', 1984)\n",
    ",('Fluminense', 'Campeonato Carioca', 1985)\n",
    ",('Fluminense', 'Campeonato Carioca', 1995)\n",
    ",('Fluminense', 'Campeonato Carioca', 2002)\n",
    ",('Fluminense', 'Campeonato Carioca', 2005)\n",
    ",('Fluminense', 'Campeonato Carioca', 2012)\n",
    ",('Fluminense', 'Campeonato Carioca', 2022)\n",
    ",('Fluminense', 'Campeonato Carioca', 2023)\n",
    ",('Fluminense', 'Taça Guanabara', 1975)\n",
    ",('Fluminense', 'Taça Guanabara', 1983)\n",
    ",('Fluminense', 'Taça Guanabara', 1985)\n",
    ",('Fluminense', 'Taça Guanabara', 1991)\n",
    ",('Fluminense', 'Taça Guanabara', 1993)\n",
    ",('Fluminense', 'Taça Guanabara', 2012)\n",
    ",('Fluminense', 'Taça Guanabara', 2017)\n",
    ",('Fluminense', 'Taça Guanabara', 2022)\n",
    ",('Fluminense', 'Taça Guanabara', 2023)\n",
    ",('Fluminense', 'Campeonato Brasileiro' ,1970)\n",
    ",('Fluminense', 'Campeonato Brasileiro' ,1984)\n",
    ",('Fluminense', 'Campeonato Brasileiro' ,2010)\n",
    ",('Fluminense', 'Campeonato Brasileiro' ,2012)]\n",
    "\n",
    "schema = ('Clube', 'Titulo', 'Ano')\n",
    "\n",
    "df = spark.createDataFrame(dados, schema =schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f38a8dc-b493-4e12-a4cb-53a4e294934e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Clube: string (nullable = true)\n |-- Titulo: string (nullable = true)\n |-- Ano: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b19054c6-1fa3-4078-8d35-ccd593d69f61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: ['__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattr__',\n '__getattribute__',\n '__getitem__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_collect_as_arrow',\n '_jcols',\n '_jdf',\n '_jmap',\n '_joinAsOf',\n '_jseq',\n '_lazy_rdd',\n '_repr_html_',\n '_sc',\n '_schema',\n '_session',\n '_sort_cols',\n '_sql_ctx',\n '_support_repr_html',\n '_to_corrected_pandas_type',\n 'agg',\n 'alias',\n 'approxQuantile',\n 'cache',\n 'checkpoint',\n 'coalesce',\n 'colRegex',\n 'collect',\n 'columns',\n 'corr',\n 'count',\n 'cov',\n 'createGlobalTempView',\n 'createOrReplaceGlobalTempView',\n 'createOrReplaceTempView',\n 'createTempView',\n 'crossJoin',\n 'crosstab',\n 'cube',\n 'describe',\n 'display',\n 'distinct',\n 'drop',\n 'dropDuplicates',\n 'drop_duplicates',\n 'dropna',\n 'dtypes',\n 'exceptAll',\n 'explain',\n 'fillna',\n 'filter',\n 'first',\n 'foreach',\n 'foreachPartition',\n 'freqItems',\n 'groupBy',\n 'groupby',\n 'head',\n 'hint',\n 'inputFiles',\n 'intersect',\n 'intersectAll',\n 'isEmpty',\n 'isLocal',\n 'isStreaming',\n 'is_cached',\n 'join',\n 'limit',\n 'localCheckpoint',\n 'mapInArrow',\n 'mapInPandas',\n 'melt',\n 'na',\n 'observe',\n 'orderBy',\n 'pandas_api',\n 'persist',\n 'printSchema',\n 'randomSplit',\n 'rdd',\n 'registerTempTable',\n 'repartition',\n 'repartitionByRange',\n 'replace',\n 'rollup',\n 'sameSemantics',\n 'sample',\n 'sampleBy',\n 'schema',\n 'select',\n 'selectExpr',\n 'semanticHash',\n 'show',\n 'sort',\n 'sortWithinPartitions',\n 'sparkSession',\n 'sql_ctx',\n 'stat',\n 'storageLevel',\n 'subtract',\n 'summary',\n 'tail',\n 'take',\n 'to',\n 'toDF',\n 'toJSON',\n 'toLocalIterator',\n 'toPandas',\n 'to_koalas',\n 'to_pandas_on_spark',\n 'transform',\n 'union',\n 'unionAll',\n 'unionByName',\n 'unpersist',\n 'unpivot',\n 'where',\n 'withColumn',\n 'withColumnRenamed',\n 'withColumns',\n 'withMetadata',\n 'withWatermark',\n 'write',\n 'writeStream',\n 'writeTo']"
     ]
    }
   ],
   "source": [
    "dir(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b2a8949-748e-466d-8d25-e592f2ba5047",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method toPandas in module pyspark.sql.pandas.conversion:\n\ntoPandas() -> 'PandasDataFrameLike' method of pyspark.sql.dataframe.DataFrame instance\n    Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n    \n    This is only available if Pandas is installed and available.\n    \n    .. versionadded:: 1.3.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Notes\n    -----\n    This method should only be used if the resulting Pandas ``pandas.DataFrame`` is\n    expected to be small, as all the data is loaded into the driver's memory.\n    \n    Usage with ``spark.sql.execution.arrow.pyspark.enabled=True`` is experimental.\n    \n    Examples\n    --------\n    >>> df.toPandas()  # doctest: +SKIP\n       age   name\n    0    2  Alice\n    1    5    Bob\n\n"
     ]
    }
   ],
   "source": [
    "help(df.toPandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb172b35-43f2-4989-911b-868dc26d8c70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: pyspark.sql.dataframe.DataFrame"
     ]
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7113b7c4-3677-449c-8fdc-b29a563360f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfPandas = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f008299-76cd-4bdb-95f5-631406333efb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: pandas.core.frame.DataFrame"
     ]
    }
   ],
   "source": [
    "type(dfPandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9a6c69c-b4ea-4bf4-ad6e-398d7070e336",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 46 entries, 0 to 45\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   Clube   46 non-null     object\n 1   Titulo  46 non-null     object\n 2   Ano     46 non-null     int64 \ndtypes: int64(1), object(2)\nmemory usage: 1.2+ KB\n"
     ]
    }
   ],
   "source": [
    "dfPandas.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "716eed87-4ace-4763-963b-6679a110b799",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfSpark = spark.createDataFrame(dfPandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb0c43fd-3555-4520-848a-ec61e962c2a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: 46"
     ]
    }
   ],
   "source": [
    "dfSpark.count()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "convertendo_dataframe_pyspark_pandas",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
