{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21739364-5ecd-4e03-88f8-6e148df726e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b14f4024-063a-4940-9cc7-b5fb192224c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1902b5d-de4b-48df-af68-f1fec872cf49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n\ncreateDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n    Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n    or a :class:`numpy.ndarray`.\n    \n    When ``schema`` is a list of column names, the type of each column\n    will be inferred from ``data``.\n    \n    When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n    from ``data``, which should be an RDD of either :class:`Row`,\n    :class:`namedtuple`, or :class:`dict`.\n    \n    When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n    the real data, or an exception will be thrown at runtime. If the given schema is not\n    :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n    :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n    Each record will also be wrapped into a tuple, which can be converted to row later.\n    \n    If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n    rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n    \n    .. versionadded:: 2.0.0\n    \n    .. versionchanged:: 2.1.0\n       Added verifySchema.\n    \n    Parameters\n    ----------\n    data : :class:`RDD` or iterable\n        an RDD of any kind of SQL data representation (:class:`Row`,\n        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n        :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n        column names, default is None.  The data type string format equals to\n        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n        omit the ``struct<>``.\n    samplingRatio : float, optional\n        the sample ratio of rows used for inferring\n    verifySchema : bool, optional\n        verify data types of every row against schema. Enabled by default.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n    \n    Notes\n    -----\n    Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n    \n    Examples\n    --------\n    >>> l = [('Alice', 1)]\n    >>> spark.createDataFrame(l).collect()\n    [Row(_1='Alice', _2=1)]\n    >>> spark.createDataFrame(l, ['name', 'age']).collect()\n    [Row(name='Alice', age=1)]\n    \n    >>> d = [{'name': 'Alice', 'age': 1}]\n    >>> spark.createDataFrame(d).collect()\n    [Row(age=1, name='Alice')]\n    \n    >>> rdd = sc.parallelize(l)\n    >>> spark.createDataFrame(rdd).collect()\n    [Row(_1='Alice', _2=1)]\n    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n    >>> df.collect()\n    [Row(name='Alice', age=1)]\n    \n    >>> from pyspark.sql import Row\n    >>> Person = Row('name', 'age')\n    >>> person = rdd.map(lambda r: Person(*r))\n    >>> df2 = spark.createDataFrame(person)\n    >>> df2.collect()\n    [Row(name='Alice', age=1)]\n    \n    >>> from pyspark.sql.types import *\n    >>> schema = StructType([\n    ...    StructField(\"name\", StringType(), True),\n    ...    StructField(\"age\", IntegerType(), True)])\n    >>> df3 = spark.createDataFrame(rdd, schema)\n    >>> df3.collect()\n    [Row(name='Alice', age=1)]\n    \n    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n    [Row(name='Alice', age=1)]\n    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n    [Row(0=1, 1=2)]\n    \n    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n    [Row(a='Alice', b=1)]\n    >>> rdd = rdd.map(lambda row: row[1])\n    >>> spark.createDataFrame(rdd, \"int\").collect()\n    [Row(value=1)]\n    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    Py4JJavaError: ...\n\n"
     ]
    }
   ],
   "source": [
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5107fc2c-4eca-4979-bc3f-bd84eeda216a",
     "showTitle": true,
     "title": "Modo 01 CreateDataFrame() "
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+\n|              _1| _2|\n+----------------+---+\n|            Fred| 39|\n|         Romário| 57|\n|  Roberto Carlos| 50|\n|Ronaldo Fenômeno| 47|\n+----------------+---+\n\n"
     ]
    }
   ],
   "source": [
    "dados = [('Fred',39)\n",
    ",('Romário', 57)\n",
    ",('Roberto Carlos', 50)\n",
    ",('Ronaldo Fenômeno', 47)]\n",
    "\n",
    "df = spark.createDataFrame(dados)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32f3bbf6-c96a-4a71-bdef-0fda4fa4dadb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n|            nome|idade|\n+----------------+-----+\n|            Fred|   39|\n|         Romário|   57|\n|  Roberto Carlos|   50|\n|Ronaldo Fenômeno|   47|\n+----------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "dados = [('Fred',39)\n",
    ",('Romário', 57)\n",
    ",('Roberto Carlos', 50)\n",
    ",('Ronaldo Fenômeno', 47)]\n",
    "\n",
    "schema = ['nome','idade']\n",
    "\n",
    "df = spark.createDataFrame(data=dados, schema=schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c980d24e-53e0-436e-b567-683c029c50b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n|            nome|idade|\n+----------------+-----+\n|            Fred|   39|\n|         Romário|   57|\n|  Roberto Carlos|   50|\n|Ronaldo Fenômeno|   47|\n+----------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "dados = [('Fred',39)\n",
    ",('Romário', 57)\n",
    ",('Roberto Carlos', 50)\n",
    ",('Ronaldo Fenômeno', 47)]\n",
    "\n",
    "schema = ['nome','idade']\n",
    "\n",
    "df = spark.createDataFrame(dados, ['nome','idade'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0e43884-e1fb-477c-abf2-e82b2d5e0d57",
     "showTitle": true,
     "title": "Modo 02 CreateDataFrame() "
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n|         Jogador|idade|\n+----------------+-----+\n|            Fred|   39|\n|         Romário|   57|\n|  Roberto Carlos|   50|\n|Ronaldo Fenômeno|   47|\n+----------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "dados = [{'Jogador':'Fred', 'idade' : 39}\n",
    ",{'Jogador':'Romário', 'idade' : 57}\n",
    ",{'Jogador':'Roberto Carlos', 'idade' : 50}\n",
    ",{'Jogador':'Ronaldo Fenômeno', 'idade' : 47}]\n",
    "\n",
    "df2 = spark.createDataFrame(data=dados)\n",
    "\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc9ce588-ee6c-4b5a-b42e-16d6914a2d39",
     "showTitle": true,
     "title": "Modo 03 CreateDataFrame() "
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StructType in module pyspark.sql.types:\n\nclass StructType(DataType)\n |  StructType(fields: Optional[List[pyspark.sql.types.StructField]] = None)\n |  \n |  Struct type, consisting of a list of :class:`StructField`.\n |  \n |  This is the data type representing a :class:`Row`.\n |  \n |  Iterating a :class:`StructType` will iterate over its :class:`StructField`\\s.\n |  A contained :class:`StructField` can be accessed by its name or position.\n |  \n |  Examples\n |  --------\n |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n |  >>> struct1[\"f1\"]\n |  StructField('f1', StringType(), True)\n |  >>> struct1[0]\n |  StructField('f1', StringType(), True)\n |  \n |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n |  >>> struct1 == struct2\n |  True\n |  >>> struct1 = StructType([StructField(\"f1\", CharType(10), True)])\n |  >>> struct2 = StructType([StructField(\"f1\", CharType(10), True)])\n |  >>> struct1 == struct2\n |  True\n |  >>> struct1 = StructType([StructField(\"f1\", VarcharType(10), True)])\n |  >>> struct2 = StructType([StructField(\"f1\", VarcharType(10), True)])\n |  >>> struct1 == struct2\n |  True\n |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n |  ...     StructField(\"f2\", IntegerType(), False)])\n |  >>> struct1 == struct2\n |  False\n |  \n |  Method resolution order:\n |      StructType\n |      DataType\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __getitem__(self, key: Union[str, int]) -> pyspark.sql.types.StructField\n |      Access fields by name or slice.\n |  \n |  __init__(self, fields: Optional[List[pyspark.sql.types.StructField]] = None)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __iter__(self) -> Iterator[pyspark.sql.types.StructField]\n |      Iterate the fields\n |  \n |  __len__(self) -> int\n |      Return the number of fields.\n |  \n |  __repr__(self) -> str\n |      Return repr(self).\n |  \n |  add(self, field: Union[str, pyspark.sql.types.StructField], data_type: Union[str, pyspark.sql.types.DataType, NoneType] = None, nullable: bool = True, metadata: Optional[Dict[str, Any]] = None) -> 'StructType'\n |      Construct a :class:`StructType` by adding new elements to it, to define the schema.\n |      The method accepts either:\n |      \n |          a) A single parameter which is a :class:`StructField` object.\n |          b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n |             metadata(optional). The data_type parameter may be either a String or a\n |             :class:`DataType` object.\n |      \n |      Parameters\n |      ----------\n |      field : str or :class:`StructField`\n |          Either the name of the field or a :class:`StructField` object\n |      data_type : :class:`DataType`, optional\n |          If present, the DataType of the :class:`StructField` to create\n |      nullable : bool, optional\n |          Whether the field to add should be nullable (default True)\n |      metadata : dict, optional\n |          Any additional metadata (default None)\n |      \n |      Returns\n |      -------\n |      :class:`StructType`\n |      \n |      Examples\n |      --------\n |      >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True), \\\n |      ...     StructField(\"f2\", StringType(), True, None)])\n |      >>> struct1 == struct2\n |      True\n |      >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n |      >>> struct1 == struct2\n |      True\n |      >>> struct1 = StructType().add(\"f1\", \"string\", True)\n |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n |      >>> struct1 == struct2\n |      True\n |  \n |  fieldNames(self) -> List[str]\n |      Returns all field names in a list.\n |      \n |      Examples\n |      --------\n |      >>> struct = StructType([StructField(\"f1\", StringType(), True)])\n |      >>> struct.fieldNames()\n |      ['f1']\n |  \n |  fromInternal(self, obj: Tuple) -> 'Row'\n |      Converts an internal SQL object into a native Python object.\n |  \n |  jsonValue(self) -> Dict[str, Any]\n |  \n |  needConversion(self) -> bool\n |      Does this type needs conversion between Python object and internal SQL object.\n |      \n |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n |  \n |  simpleString(self) -> str\n |  \n |  toInternal(self, obj: Tuple) -> Tuple\n |      Converts a Python object into an internal SQL object.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  fromJson(json: Dict[str, Any]) -> 'StructType' from builtins.type\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from DataType:\n |  \n |  __eq__(self, other: Any) -> bool\n |      Return self==value.\n |  \n |  __hash__(self) -> int\n |      Return hash(self).\n |  \n |  __ne__(self, other: Any) -> bool\n |      Return self!=value.\n |  \n |  json(self) -> str\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from DataType:\n |  \n |  typeName() -> str from builtins.type\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from DataType:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n"
     ]
    }
   ],
   "source": [
    "help(StructType)\n",
    "#StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2bb2249-a3f3-4bb9-8227-22761fd00d95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n|         jogador|idade|\n+----------------+-----+\n|            Fred|   39|\n|         Romário|   57|\n|  Roberto Carlos|   50|\n|Ronaldo Fenômeno|   47|\n+----------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "dados = [('Fred', 39)\n",
    ",('Romário',57)\n",
    ",('Roberto Carlos',50)\n",
    ",('Ronaldo Fenômeno',47)]\n",
    "\n",
    "schema = StructType([StructField('jogador', StringType(), True)\n",
    "            ,StructField('idade', IntegerType(), True)])\n",
    "\n",
    "df = spark.createDataFrame(data=dados, schema=schema)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43a45ebc-4ac3-437b-af01-3990ab6bb912",
     "showTitle": true,
     "title": "Modo 04 CreateDataFrame() "
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n|            nome|idade|\n+----------------+-----+\n|            Fred|   39|\n|         Romário|   57|\n|  Roberto Carlos|   50|\n|Ronaldo Fenômeno|   47|\n+----------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "dados = [('Fred',39)\n",
    ",('Romário', 57)\n",
    ",('Roberto Carlos', 50)\n",
    ",('Ronaldo Fenômeno', 47)]\n",
    "\n",
    "df = spark.createDataFrame(dados, \"nome:string,idade:string\")\n",
    "\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "createDataFrame",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
